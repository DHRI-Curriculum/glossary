# Tokenizing

A process of breaking text into individual linguistic units. This process often involves removing punctuation and making all words lowercase. 